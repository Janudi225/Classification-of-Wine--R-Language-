BSS = BSS,
TSS = TSS,
WSS = WSS,
BSS_over_TSS = BSS_over_TSS
)
}
# Function to calculate silhouette width and plot
calculate_silhouette <- function(kmeans_result, data) {
# Calculate the silhouette result
silhouette_result <- silhouette(kmeans_result$cluster, dist(data))
# Calculate the average silhouette width
avg_silhouette_width <- mean(silhouette_result[, 2])
# Print average silhouette width
print(paste("Average Silhouette Width Score:", avg_silhouette_width))
# Plot silhouette width
silhouette_plot <- fviz_silhouette(silhouette_result) +
ggtitle("Silhouette Plot")
# Return the plot and average silhouette width
list(
silhouette_plot = silhouette_plot,
avg_silhouette_width = avg_silhouette_width
)
}
# Function to calculate the Calinski-Harabasz index for k-means clustering results
calculate_calinski_harabasz <- function(kmeans_result, data) {
calinski_harabasz_index <- cluster.stats(dist(data), kmeans_result$cluster)$ch
return(calinski_harabasz_index)
}
# Load the dataset
whiteWine <- read_excel("Whitewine_v6.xlsx")
# Create a subset of only the first 11 columns
whiteWine_subset <- whiteWine[, 1:11]
# Scale the subset
whiteWine_subset_scaled <- as.data.frame(scale(whiteWine_subset))
# Define the z-score threshold
z_threshold <- 2
# Clean rows exceeding the threshold
whiteWine_subset_cleaned <- whiteWine_subset_scaled %>%
filter(rowSums(across(everything(), ~ abs(.) > z_threshold)) == 0)
boxplot(whiteWine_subset_cleaned)
# Perform PCA and get transformed data
pca_result <- perform_pca(whiteWine_subset_cleaned)
head(pca_result)
# Find the optimal number of clusters
optimal_k <- find_optimal_clusters(pca_result$transformed_data, max_clusters = 10)
#Define the value of k
k=2
# Perform k-means clustering and calculate metrics
kmeans_results <- perform_kmeans_clustering(pca_result$transformed_data, k)
kmeans_results
#Print the cluster centers
print("Cluster Centers:")
print(kmeans_results$kmeans_result$centers)
# Calculate silhouette and plot
silhouette_info <- calculate_silhouette(kmeans_results$kmeans_result, pca_result$transformed_data)
# Plot silhouette plot
print(silhouette_info$silhouette_plot)
# Plot k-means clustering results
fviz_cluster(kmeans_results$kmeans_result, data = pca_result$transformed_data, ellipse.type = "convex") +
ggtitle(paste("K-Means Clustering with k =", optimal_k))
# Calculate the Calinski-Harabasz index
calinski_harabasz_index <- calculate_calinski_harabasz(kmeans_results$kmeans_result, pca_result$transformed_data)
print(paste("Calinski-Harabasz Index:", calinski_harabasz_index))
# Plotting Calinski-Harabasz index for different k values
k_values <- 2:10  # Define a range of k values
calinski_harabasz_values <- sapply(k_values, function(k) {
# Perform k-means clustering with k clusters
kmeans_temp <- kmeans(pca_result$transformed_data, centers = k, nstart = 25)
# Calculate the Calinski-Harabasz index
cluster.stats(dist(pca_result$transformed_data), kmeans_temp$cluster)$ch
})
# Plot Calinski-Harabasz index
plot(k_values, calinski_harabasz_values, type = "b", xlab = "Number of Clusters (k)",
ylab = "Calinski-Harabasz Index", col = "blue", pch = 19, cex = 1.2)
title("Calinski-Harabasz Index for Different Numbers of Clusters")
dev.off()
# Define the lagged input features up to t-4
lags <- 4  # Number of lags up to t-4
# Load necessary libraries
library(readxl)
library(neuralnet)
library(Metrics)
library(ggplot2)
# Load the dataset
data <- read_excel("ExchangeUSD.xlsx", col_types = c("date", "numeric", "numeric"))
# Select only the 3rd column (USD/EUR exchange rates)
exchange_rates <- data[[3]]
# Define the lagged input features up to t-4
lags <- 4  # Number of lags up to t-4
# Create input/output matrices
input_features <- data.frame(matrix(nrow = length(exchange_rates) - lags, ncol = lags))
output_rates <- exchange_rates[(lags + 1):length(exchange_rates)]
# Fill in the input features with lagged data
for (i in 1:lags) {
input_features[, i] <- exchange_rates[(lags + 1 - i):(length(exchange_rates) - i)]
}
# Rename columns of the input features
colnames(input_features) <- paste0("Lag_", 1:lags)
# Combine the input and output data into a single data frame
data_combined <- cbind(input_features, Output = output_rates)
head(data_combined)
# Split the data into training and testing sets
train_size <- 400
train_data <- data_combined[1:train_size, ]
test_data <- data_combined[(train_size + 1):nrow(data_combined), ]
# Normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# Normalize the data
train_data <- as.data.frame(lapply(train_data, normalize))
summary(train_data)
test_data <- as.data.frame(lapply(test_data, normalize))
summary(test_data)
nn_model_1 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_1)
nn_model_2 <- neuralnet(Output ~ Lag_1 ,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_2)
nn_model_3 <- neuralnet(Output ~ Lag_1 + Lag_2,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_3)
nn_model_4 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_4)
nn_model_5 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_5)
nn_model_6 <- neuralnet(Output ~ Lag_1,,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_6)
nn_model_7 <- neuralnet(Output ~ Lag_1 + Lag_2 ,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_7)
nn_model_8 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_8)
nn_model_9 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_9)
nn_model_10 <- neuralnet(Output ~ Lag_1,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_10)
nn_model_11 <- neuralnet(Output ~ Lag_1+ Lag_2,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_11)
nn_model_12 <- neuralnet(Output ~ Lag_1+ Lag_2+ Lag_3,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_12)
nn_model_13 <- neuralnet(Output ~ Lag_1+ Lag_2+ Lag_3+Lag_4,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_13)
nn_model_14 <- neuralnet(Output ~ Lag_1,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_14)
nn_model_15 <- neuralnet(Output ~ Lag_1+ Lag_2,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_15)
# Define a function to calculate confusion metrics
calculate_confusion_metrics <- function(model, test_data) {
# Make predictions on the test data using the model
predictions <- compute(model, test_data[, -ncol(test_data)])$net.result
# Actual output values
actual_values <- test_data$Output
# Calculate confusion metrics
rmse_value <- rmse(actual_values, predictions)
mae_value <- mae(actual_values, predictions)
mape_value <- mape(actual_values, predictions)
smape_value <- smape(actual_values, predictions)
# Return the metrics as a list
return(list(rmse = rmse_value, mae = mae_value, mape = mape_value, smape = smape_value))
}
# List to store confusion metrics for each model
confusion_metrics_list <- list()
# Calculate confusion metrics for each model
for (i in 1:15) {
# Get the model variable
model_var <- paste0("nn_model_", i)
model <- get(model_var)
# Calculate confusion metrics
confusion_metrics <- calculate_confusion_metrics(model, test_data)
# Store error confusion in the list
confusion_metrics_list[[model_var]] <- confusion_metrics
}
# Print confusion metrics for each model
for (model_var in names(confusion_metrics_list)) {
cat("\nError metrics for", model_var, ":\n")
cat("RMSE:", confusion_metrics_list[[model_var]]$rmse, "\n")
cat("MAE:", confusion_metrics_list[[model_var]]$mae, "\n")
cat("MAPE:", confusion_metrics_list[[model_var]]$mape, "\n")
cat("SMAPE:", confusion_metrics_list[[model_var]]$smape, "\n")
}
# Identify the best model (e.g., based on lowest RMSE)
best_model_name <- names(confusion_metrics_list)[which.min(sapply(confusion_metrics_list, function(x) x$rmse))]
# Retrieve the best model
best_model <- get(best_model_name)
# Make predictions on the test data
predictions <- compute(best_model, test_data[, -ncol(test_data)])$net.result
# Actual output values
actual_values <- test_data$Output
# Combine actual and predicted values into a data frame
results <- data.frame(actual_values, predictions)
# Visualize the actual vs. predicted values using ggplot2
ggplot(results, aes(x = actual_values, y = predictions)) +
geom_point(color = "blue") +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
labs(x = "Actual Output", y = "Predicted Output", title = paste("Actual vs. Predicted Output for", best_model_name)) +
theme_minimal()
# Plot the architecture of the best model
plot(best_model)
dev.off()
# Load libraries
library(readxl)
library(dplyr)
library(factoextra)
library(cluster)
library(NbClust)
library(fpc)
# Function to perform PCA and select components based on variance threshold
perform_pca <- function(data, variance_threshold = 0.85) {
pca_result <- prcomp(data, scale. = TRUE)
#Calculate eigen values
eigenvalues <- pca_result$sdev^2
print("Eigenvalues:")
print(eigenvalues)
#Calculate eigen vectors
eigenvectors<-pca_result$rotation
#Print eigen vectors
print(eigenvectors)
#Calculate cumulative score per PC
cumulative_score <- cumsum(eigenvalues) / sum(eigenvalues)
#Print cumulative scores
print(cumulative_score)
#Plot scree plot
plot((eigenvalues) / sum(eigenvalues),ylab="Cumulative Score",type="b",main="Scree Plot")
#Determining the number of PCs needed for 85% of variance
num_pcs <- which(cumulative_score >= variance_threshold)[1]
#Transform data with selected PCs
transformed_data <- pca_result$x[, 1:num_pcs]
head(transformed_data)
list(transformed_data = transformed_data, num_pcs = num_pcs)
}
find_optimal_clusters <- function(data, max_clusters = 10) {
# Calculate and plot elbow method
elbow_plot <- fviz_nbclust(data, kmeans, method = "wss") +
ggtitle("Elbow Method")
print(elbow_plot)
# Calculate and plot silhouette method
silhouette_plot <- fviz_nbclust(data, kmeans, method = "silhouette") +
ggtitle("Silhouette Method")
print(silhouette_plot)
# Calculate gap statistic
gap_stat <- clusGap(data, FUN = kmeans, K.max = max_clusters, nstart = 25)
gap_stat_plot <- fviz_gap_stat(gap_stat) +
ggtitle("Gap Statistic")
print(gap_stat_plot)
# Find the optimal number of clusters using NbClust
nb_result <- NbClust(data = data, distance = "euclidean",
min.nc = 2, max.nc = max_clusters, method = "kmeans")
optimal_k <- nb_result$Best.nc[1]
# Return optimal number of clusters based on these methods
return(optimal_k)
}
# Function to perform k-means clustering and calculate metrics
perform_kmeans_clustering <- function(data, k) {
kmeans_result <- kmeans(data, centers = k, nstart = 25)
BSS <- kmeans_result$betweenss
TSS <- kmeans_result$totss
WSS <- kmeans_result$tot.withinss
BSS_over_TSS <- BSS / TSS
list(
kmeans_result = kmeans_result,
BSS = BSS,
TSS = TSS,
WSS = WSS,
BSS_over_TSS = BSS_over_TSS
)
}
# Function to calculate silhouette width and plot
calculate_silhouette <- function(kmeans_result, data) {
# Calculate the silhouette result
silhouette_result <- silhouette(kmeans_result$cluster, dist(data))
# Calculate the average silhouette width
avg_silhouette_width <- mean(silhouette_result[, 2])
# Print average silhouette width
print(paste("Average Silhouette Width Score:", avg_silhouette_width))
# Plot silhouette width
silhouette_plot <- fviz_silhouette(silhouette_result) +
ggtitle("Silhouette Plot")
# Return the plot and average silhouette width
list(
silhouette_plot = silhouette_plot,
avg_silhouette_width = avg_silhouette_width
)
}
# Function to calculate the Calinski-Harabasz index for k-means clustering results
calculate_calinski_harabasz <- function(kmeans_result, data) {
calinski_harabasz_index <- cluster.stats(dist(data), kmeans_result$cluster)$ch
return(calinski_harabasz_index)
}
# Load the dataset
whiteWine <- read_excel("Whitewine_v6.xlsx")
# Create a subset of only the first 11 columns
whiteWine_subset <- whiteWine[, 1:11]
# Scale the subset
whiteWine_subset_scaled <- as.data.frame(scale(whiteWine_subset))
# Define the z-score threshold
z_threshold <- 2
# Clean rows exceeding the threshold
whiteWine_subset_cleaned <- whiteWine_subset_scaled %>%
filter(rowSums(across(everything(), ~ abs(.) > z_threshold)) == 0)
boxplot(whiteWine_subset_cleaned)
# Perform PCA and get transformed data
pca_result <- perform_pca(whiteWine_subset_cleaned)
head(pca_result)
# Find the optimal number of clusters
optimal_k <- find_optimal_clusters(pca_result$transformed_data, max_clusters = 10)
#Define the value of k
k=2
# Perform k-means clustering and calculate metrics
kmeans_results <- perform_kmeans_clustering(pca_result$transformed_data, k)
kmeans_results
#Print the cluster centers
print("Cluster Centers:")
print(kmeans_results$kmeans_result$centers)
# Calculate silhouette and plot
silhouette_info <- calculate_silhouette(kmeans_results$kmeans_result, pca_result$transformed_data)
# Plot silhouette plot
print(silhouette_info$silhouette_plot)
# Plot k-means clustering results
fviz_cluster(kmeans_results$kmeans_result, data = pca_result$transformed_data, ellipse.type = "convex") +
ggtitle(paste("K-Means Clustering with k =", optimal_k))
# Calculate the Calinski-Harabasz index
calinski_harabasz_index <- calculate_calinski_harabasz(kmeans_results$kmeans_result, pca_result$transformed_data)
print(paste("Calinski-Harabasz Index:", calinski_harabasz_index))
# Plotting Calinski-Harabasz index for different k values
k_values <- 2:10  # Define a range of k values
calinski_harabasz_values <- sapply(k_values, function(k) {
# Perform k-means clustering with k clusters
kmeans_temp <- kmeans(pca_result$transformed_data, centers = k, nstart = 25)
# Calculate the Calinski-Harabasz index
cluster.stats(dist(pca_result$transformed_data), kmeans_temp$cluster)$ch
})
# Plot Calinski-Harabasz index
plot(k_values, calinski_harabasz_values, type = "b", xlab = "Number of Clusters (k)",
ylab = "Calinski-Harabasz Index", col = "blue", pch = 19, cex = 1.2)
title("Calinski-Harabasz Index for Different Numbers of Clusters")
dev.off()
# Load necessary libraries
library(readxl)
library(neuralnet)
library(Metrics)
library(ggplot2)
# Load the dataset
data <- read_excel("ExchangeUSD.xlsx", col_types = c("date", "numeric", "numeric"))
# Select only the 3rd column (USD/EUR exchange rates)
exchange_rates <- data[[3]]
# Define the lagged input features up to t-4
lags <- 4  # Number of lags up to t-4
# Create input/output matrices
input_features <- data.frame(matrix(nrow = length(exchange_rates) - lags, ncol = lags))
output_rates <- exchange_rates[(lags + 1):length(exchange_rates)]
# Fill in the input features with lagged data
for (i in 1:lags) {
input_features[, i] <- exchange_rates[(lags + 1 - i):(length(exchange_rates) - i)]
}
# Rename columns of the input features
colnames(input_features) <- paste0("Lag_", 1:lags)
# Combine the input and output data into a single data frame
data_combined <- cbind(input_features, Output = output_rates)
head(data_combined)
# Split the data into training and testing sets
train_size <- 400
train_data <- data_combined[1:train_size, ]
test_data <- data_combined[(train_size + 1):nrow(data_combined), ]
# Normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# Normalize the data
train_data <- as.data.frame(lapply(train_data, normalize))
summary(train_data)
test_data <- as.data.frame(lapply(test_data, normalize))
summary(test_data)
nn_model_1 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_1)
nn_model_2 <- neuralnet(Output ~ Lag_1 ,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_2)
nn_model_3 <- neuralnet(Output ~ Lag_1 + Lag_2,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_3)
nn_model_4 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3,
data = train_data, hidden = c(5),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_4)
nn_model_5 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_5)
nn_model_6 <- neuralnet(Output ~ Lag_1,,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_6)
nn_model_7 <- neuralnet(Output ~ Lag_1 + Lag_2 ,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_7)
nn_model_8 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3,
data = train_data, hidden = c(10),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_8)
nn_model_9 <- neuralnet(Output ~ Lag_1 + Lag_2 + Lag_3+Lag_4,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_9)
nn_model_10 <- neuralnet(Output ~ Lag_1,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_10)
nn_model_11 <- neuralnet(Output ~ Lag_1+ Lag_2,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_11)
nn_model_12 <- neuralnet(Output ~ Lag_1+ Lag_2+ Lag_3,
data = train_data, hidden = c(5,2),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_12)
nn_model_13 <- neuralnet(Output ~ Lag_1+ Lag_2+ Lag_3+Lag_4,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_13)
nn_model_14 <- neuralnet(Output ~ Lag_1,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_14)
nn_model_15 <- neuralnet(Output ~ Lag_1+ Lag_2,
data = train_data, hidden = c(5,4),
linear.output = TRUE, act.fct = "logistic")
plot(nn_model_15)
# Define a function to calculate confusion metrics
calculate_confusion_metrics <- function(model, test_data) {
# Make predictions on the test data using the model
predictions <- compute(model, test_data[, -ncol(test_data)])$net.result
# Actual output values
actual_values <- test_data$Output
# Calculate confusion metrics
rmse_value <- rmse(actual_values, predictions)
mae_value <- mae(actual_values, predictions)
mape_value <- mape(actual_values, predictions)
smape_value <- smape(actual_values, predictions)
# Return the metrics as a list
return(list(rmse = rmse_value, mae = mae_value, mape = mape_value, smape = smape_value))
}
# List to store confusion metrics for each model
confusion_metrics_list <- list()
# Calculate confusion metrics for each model
for (i in 1:15) {
# Get the model variable
model_var <- paste0("nn_model_", i)
model <- get(model_var)
# Calculate confusion metrics
confusion_metrics <- calculate_confusion_metrics(model, test_data)
# Store error confusion in the list
confusion_metrics_list[[model_var]] <- confusion_metrics
}
# Print confusion metrics for each model
for (model_var in names(confusion_metrics_list)) {
cat("\nError metrics for", model_var, ":\n")
cat("RMSE:", confusion_metrics_list[[model_var]]$rmse, "\n")
cat("MAE:", confusion_metrics_list[[model_var]]$mae, "\n")
cat("MAPE:", confusion_metrics_list[[model_var]]$mape, "\n")
cat("SMAPE:", confusion_metrics_list[[model_var]]$smape, "\n")
}
# Identify the best model (e.g., based on lowest RMSE)
best_model_name <- names(confusion_metrics_list)[which.min(sapply(confusion_metrics_list, function(x) x$rmse))]
# Retrieve the best model
best_model <- get(best_model_name)
# Make predictions on the test data
predictions <- compute(best_model, test_data[, -ncol(test_data)])$net.result
# Actual output values
actual_values <- test_data$Output
# Combine actual and predicted values into a data frame
results <- data.frame(actual_values, predictions)
# Visualize the actual vs. predicted values using ggplot2
ggplot(results, aes(x = actual_values, y = predictions)) +
geom_point(color = "blue") +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
labs(x = "Actual Output", y = "Predicted Output", title = paste("Actual vs. Predicted Output for", best_model_name)) +
theme_minimal()
# Plot the architecture of the best model
plot(best_model)
